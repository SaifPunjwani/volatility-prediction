{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"sJ2aEk7BbuAS","executionInfo":{"status":"ok","timestamp":1690247348586,"user_tz":240,"elapsed":12,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["import sklearn.decomposition\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.mixture import GaussianMixture\n","import plotly.express as px\n","import seaborn as sns\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"Gz21M8BHbuAV"},"source":["This is only for the stock `stock_id=0`."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"9o6nH8ufbuAW","outputId":"facece1a-1e33-40f7-fdd1-65f09332edbc","executionInfo":{"status":"error","timestamp":1690247348586,"user_tz":240,"elapsed":10,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-0f737788a2fa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbook_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/book_train.parquet/stock_id=0/c439ef22282f412ba39e9137a3fdabac.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrade_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/trade_train.parquet/stock_id=0/ef805fd82ff54fadb363094e3b122ab9.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# the target_df contains all stocks. The following line restricts it to stock_id=0 and drops the label, since it's not relevant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_blocks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         path_or_handle, handles, kwargs[\"filesystem\"] = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesystem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/book_train.parquet/stock_id=0/c439ef22282f412ba39e9137a3fdabac.parquet'"]}],"source":["book_df = pd.read_parquet(\"./data/book_train.parquet/stock_id=0/c439ef22282f412ba39e9137a3fdabac.parquet\")\n","trade_df = pd.read_parquet(\"./data/trade_train.parquet/stock_id=0/ef805fd82ff54fadb363094e3b122ab9.parquet\")\n","\n","target_df = pd.read_csv(\"./data/train.csv\")\n","# the target_df contains all stocks. The following line restricts it to stock_id=0 and drops the label, since it's not relevant\n","target_df = target_df[target_df.stock_id == 0].drop('stock_id', axis=1)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"_w_C5CqtEbif","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690247434205,"user_tz":240,"elapsed":1820,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}},"outputId":"21de9900-8705-4bdf-f047-8a86e4e74b60"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["try:\n","    from google.colab import drive\n","    in_colab = True\n","except:\n","    in_colab = False\n","\n","if in_colab:\n","    drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLbZisf4jv-d","executionInfo":{"status":"aborted","timestamp":1690247348587,"user_tz":240,"elapsed":8,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["book_df = pd.read_parquet(\"stock0_train.parquet\")\n","trade_df = pd.read_parquet(\"stock0_trade.parquet\")\n","\n","target_df = pd.read_parquet(\"stock0_training.parquet\")"]},{"cell_type":"markdown","metadata":{"id":"B_2QlG_rbuAX"},"source":["In the following `book_df`, there are many 10-minute windows/buckets of data. The row number is meaningless. Each `time_id` corresponds to *one* bucket, and `seconds_in_bucket` is how far into that bucket the observation is. There are gaps in `time_id` and `seconds_in_bucket`. The gaps in `seconds_in_bucket` means that we do not have access to a complete second-by-second record of the order book. Many records are left out. In this dataframe, there are 8 features once the `time_id` and `seconds_in_bucket` have been specified. Note that `seconds_in_bucket` 0 is always present, but all other times may or may not be. In fact, I checked, and every second other than 0 is missing in at least one bucket. And every second from 0-599 is present in at least one bucket."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQH-50gdbuAY","executionInfo":{"status":"aborted","timestamp":1690247348587,"user_tz":240,"elapsed":26734,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["book_df.head()"]},{"cell_type":"markdown","metadata":{"id":"VdDxliWO6R0e"},"source":["Plotting for book_df using only one time ID of 5."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82fA8z_F855a","executionInfo":{"status":"aborted","timestamp":1690247348734,"user_tz":240,"elapsed":8,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["# Converting into dataframe\n","book_plot = pd.DataFrame(book_df)\n","\n","# Select data for a specific time ID\n","time_data = book_plot[book_plot['time_id'] == 5]\n","\n","# Create the line plot for bid_price1\n","plt.plot(time_data['seconds_in_bucket'], time_data['bid_price1'], label='Bid Price 1')\n","\n","# Create the line plot for ask_price1\n","plt.plot(time_data['seconds_in_bucket'], time_data['ask_price1'], label='Ask Price 1')\n","\n","# Set plot labels and title\n","plt.xlabel('Seconds in Bucket')\n","plt.ylabel('Price')\n","plt.title(f'Bid Price 1 and Ask Price 1 for Time ID')\n","\n","# Add a legend\n","plt.legend()\n","\n","# Set the DPI value for higher resolution\n","dpi = 1000\n","\n","# Save the plot as a JPEG file\n","plt.savefig('book_plot.jpg', format='jpeg')\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NowY7IWmbuAZ"},"source":["The dataframe `trade_df` is very similar. Each row represents one trade. There are just 3 features here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YP_1LPrDbuAZ","executionInfo":{"status":"aborted","timestamp":1690247348735,"user_tz":240,"elapsed":9,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["trade_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CP-6zTuL855a","executionInfo":{"status":"aborted","timestamp":1690247348735,"user_tz":240,"elapsed":9,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["#Plotting for trade_df using only one time ID of 5.\n","# Select data for a specific time ID\n","time_data = trade_df[trade_df['time_id'] == 5]\n","\n","# Create the line plot for price\n","plt.plot(time_data['seconds_in_bucket'], time_data['price'], label='Price')\n","\n","# Create the line plot for size\n","plt.plot(time_data['seconds_in_bucket'], time_data['size'], label='Size')\n","\n","# Create the line plot for order count\n","plt.plot(time_data['seconds_in_bucket'], time_data['order_count'], label='Order Count')\n","\n","# Set plot labels and title\n","plt.xlabel('Seconds in Bucket')\n","plt.ylabel('Value')\n","plt.title('Trade Data for Time ID of 5')\n","\n","# Add a legend\n","plt.legend()\n","\n","# Set the DPI value for higher resolution\n","dpi = 1000\n","\n","# Save the plot as a JPEG file\n","plt.savefig('trade_df.jpg', format='jpeg')\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GEtgnbnJbuAa"},"source":["Finally, the target data in `target_df` contains the actual realized volatility information , called `target`, from 10 minutes in the future. This is what we want to predict. The dataframe has all the `stock_id`s but we only care about `stock_id=0`. The `time_id` is again the bucket we care about. I'll restrict it to `stock_id=0` for convenience."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_4KnkJobuAa","executionInfo":{"status":"aborted","timestamp":1690247348736,"user_tz":240,"elapsed":9,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["target_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IMJSagL855b","executionInfo":{"status":"aborted","timestamp":1690247348737,"user_tz":240,"elapsed":10,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["#Data Exploration for target_df\n","#Converting into dataframe\n","target_plot = pd.DataFrame(target_df)\n","\n","# Create the histogram\n","fig = go.Figure(data=[go.Histogram(x=target_plot['target'])])\n","\n","# Set plot labels and title\n","fig.update_layout(\n","    xaxis_title='Target',\n","    yaxis_title='Frequency',\n","    title='Histogram of Target'\n",")\n","\n","# Save the plot as a PNG file\n","offline.plot(fig, filename='target_plot.html', auto_open=False, image='png', image_filename='target_plot', image_width=800, image_height=600)\n","\n","# Show the interactive plot\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"fpqN4MRobuAb"},"source":["For the `book_df` dataframe, I am going to fill in the missing seconds with the data from the most recent second. For example, if is data at seconds 0, 4, and 7, then I will give seconds 1-3 the data from second 0, and seconds 5-6 the data from second 4."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5tdK1BlAbuAb","executionInfo":{"status":"aborted","timestamp":1690247348738,"user_tz":240,"elapsed":11,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["# get a list of all time buckets and seconds , without repetitions\n","unique_times = book_df['time_id'].drop_duplicates().to_frame()\n","unique_seconds = book_df['seconds_in_bucket'].drop_duplicates().sort_values().to_frame()\n","\n","# take the cross of these. So all possible (bucket, second) pairs are present\n","full_book_df = unique_times.merge(unique_seconds, how='cross')\n","\n","# filling in the seconds data requires things to be sorted by seconds\n","full_book_df = full_book_df.sort_values(['seconds_in_bucket', 'time_id'])\n","book_sorted_df = book_df.sort_values('seconds_in_bucket')\n","\n","# This actually fills things in. 'on' means fill in seconds, 'by' means do it for each time_id.\n","full_book_df = pd.merge_asof(left=full_book_df, right=book_sorted_df, on='seconds_in_bucket', by='time_id')\n","\n","# go back to our original order\n","full_book_df = full_book_df.sort_values(['time_id', 'seconds_in_bucket']).reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"1DovZNS_buAb"},"source":["Now we see that all seconds are present. There are changes at seconds 1, 5, 6."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1jUP4aNGbuAb","executionInfo":{"status":"aborted","timestamp":1690247348739,"user_tz":240,"elapsed":12,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["full_book_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQhUUkfE855b","executionInfo":{"status":"aborted","timestamp":1690247348739,"user_tz":240,"elapsed":11,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["#Visualizing full_book_df\n","# Extract the relevant columns from the dataframe\n","time_id = full_book_df['time_id']\n","bid_price1 = full_book_df['bid_price1']\n","ask_price1 = full_book_df['ask_price1']\n","bid_size1 = full_book_df['bid_size1']\n","ask_size1 = full_book_df['ask_size1']\n","\n","bid_price2 = full_book_df['bid_price2']\n","ask_price2 = full_book_df['ask_price2']\n","bid_size2 = full_book_df['bid_size2']\n","ask_size2 = full_book_df['ask_size2']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vNc0tQB855b","executionInfo":{"status":"aborted","timestamp":1690247348740,"user_tz":240,"elapsed":12,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["#Plotting for Bid Price 1 and Ask Price 1\n","# Filter the data for time_id = 5\n","filtered_data = full_book_df[full_book_df['time_id'] == 5]\n","\n","# Get the filtered bid_price1 and ask_price1 values\n","filtered_bid_price1 = filtered_data['bid_price1']\n","filtered_ask_price1 = filtered_data['ask_price1']\n","\n","# Create the figure for Bid Price 1 and Ask Price 1\n","plt.figure()\n","\n","# Plot the bid_price1 line\n","plt.plot(filtered_bid_price1, color='blue', label='Bid Price 1')\n","\n","# Plot the ask_price1 line\n","plt.plot(filtered_ask_price1, color='green', label='Ask Price 1')\n","\n","# Set plot labels and title\n","plt.xlabel('Index')\n","plt.ylabel('Price')\n","plt.title('Bid Price 1 and Ask Price 1 for Time ID 5')\n","\n","# Add a legend\n","plt.legend()\n","\n","# Set the DPI value for higher resolution\n","dpi = 1000\n","\n","# Save the plot as a JPEG file\n","plt.savefig('bid_ask_price1.jpg', format='jpeg')\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pj77mDAh6R0h"},"source":["Visualizing for bid_price2 and ask_price2. Plotting for Bid Price 2 and Ask Price 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81xuVnsE855c","executionInfo":{"status":"aborted","timestamp":1690247348741,"user_tz":240,"elapsed":13,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["# Filter the data for time_id = 5\n","filtered_data_2 = full_book_df[full_book_df['time_id'] == 5]\n","\n","# Get the filtered bid_price1 and ask_price1 values\n","filtered_bid_price2 = filtered_data_2['bid_price2']\n","filtered_ask_price2 = filtered_data_2['ask_price2']\n","\n","# Create the figure for Bid Price 1 and Ask Price 1\n","plt.figure()\n","\n","# Plot the bid_price1 line\n","plt.plot(filtered_bid_price2, color='blue', label='Bid Price 1')\n","\n","# Plot the ask_price1 line\n","plt.plot(filtered_ask_price2, color='green', label='Ask Price 1')\n","\n","# Set plot labels and title\n","plt.xlabel('Index')\n","plt.ylabel('Price')\n","plt.title('Bid Price 2 and Ask Price 2 for Time ID 5')\n","\n","# Add a legend\n","plt.legend()\n","\n","# Set the DPI value for higher resolution\n","dpi = 1000\n","\n","# Save the plot as a JPEG file\n","plt.savefig('bid_ask_price2.jpg', format='jpeg')\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMoU6jx7855c","executionInfo":{"status":"aborted","timestamp":1690247348741,"user_tz":240,"elapsed":13,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["#Visualizing for bid size 1 and ask size 1\n","# Filter the data for time_id = 5\n","filtered_data_3 = full_book_df[full_book_df['time_id'] == 5]\n","\n","# Get the filtered bid_price1 and ask_price1 values\n","filtered_bid_size1 = filtered_data_3['bid_size1']\n","filtered_ask_size1 = filtered_data_3['ask_size1']\n","\n","# Create the figure for Bid Price 1 and Ask Price 1\n","plt.figure()\n","\n","# Plot the bid_price1 line\n","plt.plot(filtered_bid_size1, color='blue', label='Bid Size 1')\n","\n","# Plot the ask_price1 line\n","plt.plot(filtered_ask_size1, color='green', label='Ask Size 1')\n","\n","# Set plot labels and title\n","plt.xlabel('Index')\n","plt.ylabel('Price')\n","plt.title('Bid Size 1 and Ask Size 1 for Time ID 5')\n","\n","# Add a legend\n","plt.legend()\n","\n","# Set the DPI value for higher resolution\n","dpi = 1000\n","\n","# Save the plot as a JPEG file\n","plt.savefig('bid_ask_size1.jpg', format='jpeg')\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3BSfyW2855c","executionInfo":{"status":"aborted","timestamp":1690247348742,"user_tz":240,"elapsed":14,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["#Visualizing for bid size 2 and ask size 2\n","# Filter the data for time_id = 5\n","filtered_data_4 = full_book_df[full_book_df['time_id'] == 5]\n","\n","# Get the filtered bid_price1 and ask_price1 values\n","filtered_bid_size2 = filtered_data_4['bid_size2']\n","filtered_ask_size2 = filtered_data_4['ask_size2']\n","\n","# Create the figure for Bid Price 1 and Ask Price 1\n","plt.figure()\n","\n","# Plot the bid_price1 line\n","plt.plot(filtered_bid_size2, color='blue', label='Bid Size 2')\n","\n","# Plot the ask_price1 line\n","plt.plot(filtered_ask_size2, color='green', label='Ask Size 2')\n","\n","# Set plot labels and title\n","plt.xlabel('Index')\n","plt.ylabel('Price')\n","plt.title('Bid Size 2 and Ask Size 2 for Time ID 5')\n","\n","# Add a legend\n","plt.legend()\n","\n","# Set the DPI value for higher resolution\n","dpi = 1000\n","\n","# Save the plot as a JPEG file\n","plt.savefig('bid_ask_size2.jpg', format='jpeg')\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2GEOOz1RbuAb"},"source":["Next, we just copy the code from the Optiver notebook to calculate the *weighted average price*, or `wap`. Since the true price is somewhere between the highest bid and lowest ask, this uses the number of bids and asks to determine where in the middle the true price is. We also, copying code from the Optiver notebook, calculate the log returns. We can do the same thing for the ordinary book. They'll be somewhat different."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KyyCd1kwbuAc","executionInfo":{"status":"aborted","timestamp":1690247348742,"user_tz":240,"elapsed":14,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["def log_return(list_stock_prices):\n","    return np.log(list_stock_prices).diff()\n","\n","full_book_df['wap'] = (full_book_df['bid_price1'] * full_book_df['ask_size1'] +\n","                                full_book_df['ask_price1'] * full_book_df['bid_size1']) / (\n","                                       full_book_df['bid_size1']+ full_book_df['ask_size1'])\n","\n","book_df['wap'] = (book_df['bid_price1'] * book_df['ask_size1'] +\n","                                book_df['ask_price1'] * book_df['bid_size1']) / (\n","                                       book_df['bid_size1']+ book_df['ask_size1'])\n","\n","full_book_df.loc[:,'log_return'] = log_return(full_book_df['wap'])\n","# at time 0, this calculation is meaningless\n","full_book_df = full_book_df[full_book_df['seconds_in_bucket'] != 0]\n","\n","book_df.loc[:,'log_return'] = log_return(book_df['wap'])\n","# at time 0, this calculation is meaningless\n","book_df = book_df[book_df['seconds_in_bucket'] != 0]"]},{"cell_type":"markdown","metadata":{"id":"LyawbPrMbuAc"},"source":["We can grab a specific bucket, given by `time_id==5`, and plot the stock price over the 10 minute window."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wFI9v4RbuAc","executionInfo":{"status":"aborted","timestamp":1690247348743,"user_tz":240,"elapsed":14,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["# Plot the data\n","plt.plot(full_book_df[full_book_df[\"time_id\"] == 5]['log_return'])\n","\n","# Set plot labels and title\n","plt.xlabel('Data Point')\n","plt.ylabel('Log Return')\n","plt.title('Log Return for Time ID 5')\n","\n","# Save the plot as a JPEG image\n","plt.savefig('log_return_plot.jpg', format='jpeg', dpi=300)\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6gAHENaYjv-n","executionInfo":{"status":"aborted","timestamp":1690247348743,"user_tz":240,"elapsed":14,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["fig = px.line(book_df.loc[book_df['time_id']==5], x=\"seconds_in_bucket\", y=\"log_return\", title='log return of book:stock_id_0, time_id=5')\n","fig.update_layout(yaxis_range=[0.0015,-0.001])\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WseoHF6jv-o","executionInfo":{"status":"aborted","timestamp":1690247348743,"user_tz":240,"elapsed":14,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["fig = px.line(full_book_df.loc[full_book_df['time_id']==5], x=\"seconds_in_bucket\", y=\"log_return\", title='log return of full book:stock_id_0, time_id=5')\n","fig.update_layout(yaxis_range=[0.0015,-0.001])\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"xALEoARgnjqH"},"source":["An important feature will be the realized volatility over the 10 minute window. We can compute this by taking\n","$$\n","    \\sqrt{\\sum_t r_t^2},\n","$$\n","where $r_t$ is the log return from time $t$ to $t+1$. I'll also add some basic features: max price, min price, average price, average `bid_size1/2`, average `ask_size1/2`, etc. See the code for details. The `lambda` functions are being used to compute volatility, and the mean/std/max of the gaps between orders in the trade dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZdymqGQnjqI","executionInfo":{"status":"aborted","timestamp":1690247348744,"user_tz":240,"elapsed":15,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["book_features_df = book_df.groupby('time_id').aggregate(\n","    vol = pd.NamedAgg('log_return', lambda x: np.sqrt(sum(a*a for a in x))), # anonymous function to compute vol from log returns\n","    mean_wap = pd.NamedAgg('wap', 'mean'),\n","    min_wap = pd.NamedAgg('wap', 'min'),\n","    max_wap = pd.NamedAgg('wap', 'max'),\n","    mean_bid1 = pd.NamedAgg('bid_size1', 'mean'),\n","    mean_bid2 = pd.NamedAgg('bid_size2', 'mean'),\n","    mean_ask1 = pd.NamedAgg('ask_size1', 'mean'),\n","    mean_ask2 = pd.NamedAgg('ask_size2', 'mean'),\n","    min_bid1 = pd.NamedAgg('bid_size1', 'min'),\n","    min_ask1 = pd.NamedAgg('ask_size1', 'min'),\n",")\n","\n","trade_features_df = trade_df.groupby('time_id').aggregate(\n","    avg_gap = pd.NamedAgg('seconds_in_bucket', lambda x: np.mean(np.diff(x))),\n","    std_gap = pd.NamedAgg('seconds_in_bucket', lambda x: np.std(np.diff(x))),\n","    max_gap = pd.NamedAgg('seconds_in_bucket', lambda x: np.max(np.diff(x))),\n","    sum_size = pd.NamedAgg('size', 'sum'),\n","    max_size = pd.NamedAgg('size', 'max'),\n","    sum_orders = pd.NamedAgg('order_count', 'sum'),\n","    max_orders = pd.NamedAgg('order_count', 'max'),\n",")\n","\n","features_df = book_features_df.join(trade_features_df, on='time_id')\n","features_df.head()"]},{"cell_type":"markdown","metadata":{"id":"B9rqZq3KbuAc"},"source":["Now let's add the Fourier transforms. We'll do PCA on top of this. The benefit of the Fourier transform is to make things shift-independent. So if a pattern happens at times 100-110, the same pattern at times 220-230 can get picked up. To do this, I'm going to put the dataframe into a numpy tensor so that we can do the Fourier transform in parallel. I'll leave it in the tensor for now for the sake of PCA."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYws-_MHbuAc","executionInfo":{"status":"aborted","timestamp":1690247348745,"user_tz":240,"elapsed":15,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["num_times = len(full_book_df['time_id'].unique())\n","num_seconds = len(full_book_df['seconds_in_bucket'].unique())\n","# the below could be useful\n","# original_col_names = list(full_book_df.columns)\n","# fourier_col_names = [\"fft_\" + name for name in original_col_names if name not in ('time_id', 'seconds_in_bucket')]\n","# new_col_names = original_col_names + fourier_col_names\n","\n","book_tensor = full_book_df.drop(['time_id', 'seconds_in_bucket'], axis=1).to_numpy(dtype='float').reshape(num_times, num_seconds, -1)\n","fourier = np.fft.fft(book_tensor, axis=1)\n","\n","# now fourier has complex entries. This isn't so nice for working with, so we'll split the complex numbers into real and imaginary\n","real, imag = np.real(fourier), np.imag(fourier)\n","fourier = np.concatenate([real, imag], axis=-1)\n","print(f\"Fourier shape: {fourier.shape}. This is datapoints, timesteps, features. Features has doubled since we have real + imaginary\")"]},{"cell_type":"markdown","metadata":{"id":"-ub2hBHNbuAd"},"source":["At this point, if we do PCA, we'll end up leaking data from the validation set. We also need to standardize our variables (again, on the training set only). So we really need to do some train/test split."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0OPz-g3buAd","executionInfo":{"status":"aborted","timestamp":1690247348746,"user_tz":240,"elapsed":26816,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["train_proportion = .8\n","train_cutoff_index = int(train_proportion*fourier.shape[0])\n","\n","fourier_train, fourier_test = np.split(fourier, [train_cutoff_index,])\n","train_mean = np.mean(fourier_train, axis=(0, 1))\n","train_std = np.std(fourier_train, axis=(0, 1))\n","normalized_train = (fourier_train-train_mean)/train_std\n","# we use the train_std and train_mean since that's what we do PCA on.\n","normalized_fourier = (fourier-train_mean)/train_std"]},{"cell_type":"markdown","metadata":{"id":"yC-JzdsWbuAd"},"source":["Now we can run PCA on the train dataset. It takes my computer about 10 seconds, so it's not too slow. If this runs out of memory, we can switch to incremental PCA. I'll keep 10 dimensions (10 was selected by an elbow method)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IghtQ6kLbuAd","executionInfo":{"status":"aborted","timestamp":1690247348746,"user_tz":240,"elapsed":26813,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["n_components = 10\n","fourier_pca = sklearn.decomposition.PCA(n_components)\n","fourier_pca.fit(normalized_train.reshape([normalized_train.shape[0], -1]))\n","\n","# if we want to visualize the amount of variance explained, we can increase n_components and then plot the following\n","# plt.plot(fourier_pca.explained_variance_ratio_)"]},{"cell_type":"markdown","metadata":{"id":"l72Z3c9Z6R0j"},"source":["PCA Explained Variance Ratio Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_A4hG3SE855d","executionInfo":{"status":"aborted","timestamp":1690247348746,"user_tz":240,"elapsed":26809,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["# Get the explained variance ratio\n","explained_variance_ratio = fourier_pca.explained_variance_ratio_\n","\n","# Calculate the cumulative explained variance\n","cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n","\n","# Plot the explained variance ratio\n","plt.plot(range(1, n_components + 1), explained_variance_ratio, marker='o')\n","plt.xlabel('Principal Components')\n","plt.ylabel('Explained Variance Ratio')\n","plt.title('Explained Variance Ratio for Fourier PCA')\n","plt.grid(True)\n","plt.savefig('explained_variance_ratio.jpeg', dpi=300)\n","plt.show()\n","\n","# Plot the cumulative explained variance\n","plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o')\n","plt.xlabel('Principal Components')\n","plt.ylabel('Cumulative Explained Variance')\n","plt.title('Cumulative Explained Variance for Fourier PCA')\n","plt.grid(True)\n","plt.savefig('cumulative_explained_variance.jpeg', dpi=300)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Upkrl565buAd"},"source":["Finally, we want to capture these components as features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgLHZk2MbuAe","executionInfo":{"status":"aborted","timestamp":1690247348747,"user_tz":240,"elapsed":26806,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["pca_features = fourier_pca.transform(normalized_fourier.reshape([normalized_fourier.shape[0], -1]))\n","\n","indices = full_book_df['time_id'].unique().reshape(-1, 1)\n","train_indices = indices[:train_cutoff_index].reshape(-1, 1)\n","test_indices = indices[train_cutoff_index:].reshape(-1, 1)\n","\n","pca_col_names = ['time_id'] + ['pca_' + str(i) for i in range(n_components)]\n","pca_features_df = pd.DataFrame(data=np.concatenate((indices, pca_features), axis=1),  columns=pca_col_names)\n","\n","# change time_id back to integer and make it an index\n","pca_features_df = pca_features_df.astype({'time_id': int})\n","pca_features_df = pca_features_df.set_index('time_id')"]},{"cell_type":"markdown","metadata":{"id":"ht1oxfDM6R0o"},"source":["PCA Scatter Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8snBtK96R0o","executionInfo":{"status":"aborted","timestamp":1690247348747,"user_tz":240,"elapsed":26802,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["# Create a scatter plot\n","plt.figure(figsize=(10, 5))\n","plt.scatter(pca_features[:, 0], pca_features[:, 1], alpha=0.5)\n","plt.title('PCA Scatter Plot')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.savefig('pca_scatter_plot.jpeg', dpi=300)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zK21ksA6LeCZ"},"source":[]},{"cell_type":"markdown","metadata":{"id":"_vNXfqSjnjqK"},"source":["We can combine this with the features we found earlier. Then we'll split into train and test (again)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_P_z7bJanjqK","executionInfo":{"status":"aborted","timestamp":1690247348748,"user_tz":240,"elapsed":26799,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["features_df = features_df.join(pca_features_df)\n","train_df = features_df.iloc[:train_cutoff_index, :]\n","test_df  = features_df.iloc[train_cutoff_index:, :]\n","\n","target_train_df = target_df.iloc[:train_cutoff_index, :]\n","target_test_df = target_df.iloc[train_cutoff_index:, :]"]},{"cell_type":"markdown","metadata":{"id":"Jc7hmthebuAe"},"source":["Now `train_df` and `test_df` contain all the PCA features and the handcrafted features. The corresponding `target_train_df` and `target_test_df` contain the volatility that we actually want to predict. Next, we implement the GMM, which can give class probabilities as a feature."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Z8bppGtnjqK","executionInfo":{"status":"aborted","timestamp":1690247348748,"user_tz":240,"elapsed":26795,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["max_components = 10  # Adjust the max number of components as desired\n","component_range = range(1, max_components+1)\n","\n","# Initialize BIC scores\n","bic_scores = []\n","\n","# Iterate over different values of n_components\n","for n in component_range:\n","    # Create and fit the GMM model\n","    gmm = GaussianMixture(n_components=n)\n","    gmm.fit(train_df)\n","\n","    # Calculate the BIC score\n","    bic = gmm.bic(train_df)\n","\n","    # Store the BIC score\n","    bic_scores.append(bic)\n","\n","# Find the optimal number of components that minimizes BIC\n","gmm_num_components = component_range[np.argmin(bic_scores)]\n","print(\"Optimal number of n components:\", gmm_num_components)"]},{"cell_type":"markdown","metadata":{"id":"yjeQrvd16R0o"},"source":["Visualizing GMM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjeHYIOE855g","executionInfo":{"status":"aborted","timestamp":1690247348748,"user_tz":240,"elapsed":26790,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["# Create the line plot for BIC scores\n","plt.plot(component_range, bic_scores, marker='o')\n","\n","# Set plot labels and title\n","plt.xlabel('Number of Components')\n","plt.ylabel('BIC Score')\n","plt.title('BIC Score vs Number of Components')\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-GUWfxHUnjqK"},"source":["Now that we have the optimal number of components, we can fit. Then we'll predict (on the test and train set, to get features) and add these probabilities to our features. We'll also drop one of the cluster probabilities from the feature list, since the final probability can just be computed from the others, since the probabilities must sum to one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcXrAuuenjqK","executionInfo":{"status":"aborted","timestamp":1690247348749,"user_tz":240,"elapsed":26787,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["gmm = GaussianMixture(n_components=gmm_num_components)\n","gmm.fit(train_df)\n","probabilities = gmm.predict_proba(features_df)\n","reduced_probabilities = np.delete(probabilities, 0, axis=1)\n","\n","indices = full_book_df['time_id'].unique().reshape(-1, 1)\n","\n","gmm_col_names = ['time_id'] + ['gmm_' + str(i) for i in range(gmm_num_components-1)]\n","gmm_features_df = pd.DataFrame(data=np.concatenate((indices, reduced_probabilities), axis=1),  columns=gmm_col_names)\n","\n","# change time_id back to integer and make it an index\n","gmm_features_df = gmm_features_df.astype({'time_id': int})\n","gmm_features_df = gmm_features_df.set_index('time_id')"]},{"cell_type":"markdown","metadata":{"id":"2AYmon8fnjqK"},"source":["Finally, combine all features into a single dataframe. Split into test and train as before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gyaW28SunjqL","executionInfo":{"status":"aborted","timestamp":1690247348749,"user_tz":240,"elapsed":26783,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["features_df = features_df.join(gmm_features_df)\n","train_df = features_df.iloc[:train_cutoff_index, :]\n","test_df  = features_df.iloc[train_cutoff_index:, :]\n","features_df.head()"]},{"cell_type":"markdown","metadata":{"id":"OU6xiI6_B-p5"},"source":["Initialize the Random Forest model. We are using the RandomForestRegressor class from sklearn.ensemble module."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pt4gRSQECL5y","executionInfo":{"status":"aborted","timestamp":1690247348749,"user_tz":240,"elapsed":26779,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["rf = RandomForestRegressor(random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"esGnIXVZCUq0"},"source":["Define the grid of hyperparameters to search. We define a dictionary where the keys are the names of the hyperparameters we want to tune, and the values are lists of the values we want to try out."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JF5hhj0uCWn2","executionInfo":{"status":"aborted","timestamp":1690247348750,"user_tz":240,"elapsed":26776,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 10, 20, 30],\n","    'min_samples_split': [2, 5, 10]\n","}"]},{"cell_type":"markdown","metadata":{"id":"42lbd_LDCbGz"},"source":["Set up the grid search. We are using the GridSearchCV class from sklearn.model_selection module. This class performs an exhaustive search over the specified parameter values for an estimator."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-Ofq0pfChqO","executionInfo":{"status":"aborted","timestamp":1690247348750,"user_tz":240,"elapsed":26771,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')"]},{"cell_type":"markdown","metadata":{"id":"PlegYCT-CqFS"},"source":["Train the model. We are fitting the grid search object to the training data. This will perform the grid search and fit the best model to the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBrXA-IZC4WV","executionInfo":{"status":"aborted","timestamp":1690247348750,"user_tz":240,"elapsed":26767,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["grid_search.fit(train_df, target_train_df['target'])"]},{"cell_type":"markdown","metadata":{"id":"n_K399RGC1va"},"source":["Print the best parameters. We are printing the best hyperparameters found by the grid search."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPTrK78QC28I","executionInfo":{"status":"aborted","timestamp":1690247348751,"user_tz":240,"elapsed":26764,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["print(\"Best parameters: \", grid_search.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"_zwdT1NtnjqQ"},"source":["Make predictions with the best model. We are getting the best estimator found by the grid search and using it to make predictions on the testing data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2wPzO9jnjqQ","executionInfo":{"status":"aborted","timestamp":1690247348752,"user_tz":240,"elapsed":26761,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["best_rf = grid_search.best_estimator_\n","predictions = best_rf.predict(test_df)"]},{"cell_type":"markdown","metadata":{"id":"fhykLvevnjqQ"},"source":["Calculate RMSE.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNENMau8njqQ","executionInfo":{"status":"aborted","timestamp":1690247348753,"user_tz":240,"elapsed":26758,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["rmse = np.sqrt(mean_squared_error(target_test_df['target'], predictions))\n","print(\"RMSE: \", rmse)"]},{"cell_type":"markdown","metadata":{"id":"wjQTVnH9njqR"},"source":["Calculate MAE.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WefsE9KhnjqR","executionInfo":{"status":"aborted","timestamp":1690247348753,"user_tz":240,"elapsed":26754,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["mae = mean_absolute_error(target_test_df['target'], predictions)\n","print(\"MAE: \", mae)"]},{"cell_type":"markdown","metadata":{"id":"OshJ4mrynjqR"},"source":["Calculate R-squared. This metric provides an indication of the goodness of fit of the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIZqCdnWnjqR","executionInfo":{"status":"aborted","timestamp":1690247348754,"user_tz":240,"elapsed":26751,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["r2 = r2_score(target_test_df['target'], predictions)\n","print(\"R-squared: \", r2)"]},{"cell_type":"markdown","metadata":{"id":"kAbpPOuD6R0q"},"source":["Random Forest Visualization. First, we have to get the most important features to do this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYjC7veK6R0q","executionInfo":{"status":"aborted","timestamp":1690247348754,"user_tz":240,"elapsed":26746,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["# Get feature importances\n","importances = best_rf.feature_importances_\n","\n","# Convert the importances into one-dimensional 1darray with corresponding df column names as axis labels\n","f_importances = pd.Series(importances, train_df.columns)\n","\n","# Sort the array in descending order of the importances\n","f_importances.sort_values(ascending=False, inplace=True)\n","\n","# Make the bar Plot from f_importances\n","f_importances.plot(x='Features', y='Importance', kind='bar', figsize=(16,9), rot=45, fontsize=15)\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.savefig('feature_importances.jpeg', dpi=300)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Tndilje6R0q","executionInfo":{"status":"aborted","timestamp":1690247348754,"user_tz":240,"elapsed":26742,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(features_df, target_df, test_size=0.2, random_state=42)\n","\n","# Train the model\n","best_rf.fit(X_train, y_train)\n","\n","# Make predictions\n","predictions = best_rf.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"-LwYeV1K6R0q"},"source":["Random Forest Actual VS Predicted Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"izRxW5A_6R0q","executionInfo":{"status":"aborted","timestamp":1690247348923,"user_tz":240,"elapsed":2,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["# Make predictions on the correct data\n","predictions = best_rf.predict(X_test)\n","\n","# Calculate the error\n","errors = y_test - predictions  # If 'y_test' is a Series\n","\n","# Create a figure and axis\n","fig, ax = plt.subplots()\n","\n","# Create a scatter plot of actual vs predicted values\n","ax.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n","\n","# Add a line for perfect prediction\n","ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n","\n","# Set labels\n","ax.set_xlabel('Actual')\n","ax.set_ylabel('Predicted')\n","ax.set_title('Actual vs Predicted Values')\n","\n","# Show the plot\n","plt.savefig('random_forest_actual_vs_predicted.jpeg', dpi=300)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"wFO_jTy96R0r"},"source":["Random Forest Histogram of Prediction Errors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xpKZ3THM6R0r","executionInfo":{"status":"aborted","timestamp":1690247348923,"user_tz":240,"elapsed":2,"user":{"displayName":"Tam Truong","userId":"00191602093069828803"}}},"outputs":[],"source":["# Create a figure and axis for error plot\n","fig, ax = plt.subplots()\n","\n","# Create a histogram of the error\n","ax.hist(errors, bins=20)\n","\n","# Set labels\n","ax.set_xlabel('Prediction Error')\n","ax.set_ylabel('Frequency')\n","ax.set_title('Random Forest Prediction Error')\n","\n","# Show the plot\n","plt.savefig('random_forest_prediction_errors.jpeg', dpi=300)\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}